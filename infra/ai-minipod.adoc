---
sidebar: sidebar 
permalink: infra/ai-minipod.html 
keywords: netapp, aipod, RAG, ai solution, design 
summary: Este documento presenta un diseño de referencia validado de NetApp AIPod para Enterprise RAG con tecnologías y capacidades combinadas de procesadores Intel Xeon 6 y soluciones de gestión de datos de NetApp .  La solución demuestra una aplicación ChatQnA que aprovecha un modelo de lenguaje amplio y brinda respuestas precisas y contextualmente relevantes a usuarios simultáneos.  Las respuestas se recuperan del repositorio de conocimiento interno de una organización a través de una tubería de inferencia RAG con espacio de aire. 
---
= NetApp AIPod Mini: Inferencia RAG empresarial con NetApp e Intel
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Este documento presenta un diseño de referencia validado de NetApp AIPod para Enterprise RAG con tecnologías y capacidades combinadas de procesadores Intel Xeon 6 y soluciones de gestión de datos de NetApp .  La solución demuestra una aplicación ChatQnA que aprovecha un modelo de lenguaje amplio y brinda respuestas precisas y contextualmente relevantes a usuarios simultáneos.  Las respuestas se recuperan del repositorio de conocimiento interno de una organización a través de una tubería de inferencia RAG con espacio de aire.

image:aipod-mini-001.png["Logotipo de Intel"]

Sathish Thyagarajan, Michael Oglesby, NetApp



== Resumen ejecutivo

Un número cada vez mayor de organizaciones están aprovechando aplicaciones de generación aumentada por recuperación (RAG) y modelos de lenguaje grandes (LLM) para interpretar las indicaciones de los usuarios y generar respuestas para aumentar la productividad y el valor comercial.  Estas indicaciones y respuestas pueden incluir texto, código, imágenes o incluso estructuras de proteínas terapéuticas recuperadas de la base de conocimiento interna de una organización, lagos de datos, repositorios de código y repositorios de documentos.  Este documento cubre el diseño de referencia de la solución NetApp AIPod Mini, que comprende almacenamiento y servidores NetApp AFF con procesadores Intel Xeon 6.  Incluye el software de gestión de datos NetApp ONTAP combinado con Intel Advanced Matrix Extensions (Intel AMX) y el software Intel AI for Enterprise Retrieval-augmented Generation (RAG) integrado en Open Platform for Enterprise AI (OPEA).  El AIPod Mini de NetApp para RAG empresarial permite a las organizaciones ampliar un LLM público a una solución de inferencia de inteligencia artificial generativa (GenAI) privada.  La solución demuestra una inferencia RAG eficiente y rentable a escala empresarial, diseñada para mejorar la confiabilidad y brindarle un mejor control sobre su información confidencial.



== Validación de socios de almacenamiento de Intel

Los servidores equipados con procesadores Intel Xeon 6 están diseñados para manejar cargas de trabajo de inferencia de IA exigentes, utilizando Intel AMX para lograr el máximo rendimiento.  Para permitir un rendimiento y una escalabilidad de almacenamiento óptimos, la solución se ha validado con éxito utilizando NetApp ONTAP, lo que permite a las empresas satisfacer las necesidades de las aplicaciones RAG.  Esta validación se realizó en servidores con procesadores Intel Xeon 6.  Intel y NetApp tienen una sólida asociación centrada en ofrecer soluciones de IA optimizadas, escalables y alineadas con los requisitos comerciales del cliente.



== Ventajas de ejecutar sistemas RAG con NetApp

Las aplicaciones RAG implican la recuperación de conocimiento de los repositorios de documentos de las empresas en varios tipos, como PDF, texto, CSV, Excel o gráficos de conocimiento.  Estos datos normalmente se almacenan en soluciones como un almacenamiento de objetos S3 o NFS local como fuente de datos.  NetApp ha sido líder en tecnologías de gestión de datos, movilidad de datos, gobernanza de datos y seguridad de datos en todo el ecosistema de borde, centro de datos y nube.  La gestión de datos de NetApp ONTAP proporciona almacenamiento de nivel empresarial para soportar varios tipos de cargas de trabajo de IA, incluidas inferencias por lotes y en tiempo real, y ofrece algunos de los siguientes beneficios:

* Velocidad y escalabilidad.  Puede manejar grandes conjuntos de datos a alta velocidad para el control de versiones con la capacidad de escalar el rendimiento y la capacidad de forma independiente.
* Acceso a datos.  La compatibilidad con múltiples protocolos permite que las aplicaciones cliente lean datos mediante los protocolos de intercambio de archivos S3, NFS y SMB.  Los buckets NAS de ONTAP S3 pueden facilitar el acceso a los datos en escenarios de inferencia LLM multimodal.
* Confiabilidad y confidencialidad.  ONTAP proporciona protección de datos, protección autónoma contra ransomware (ARP) de NetApp integrada y aprovisionamiento dinámico de almacenamiento, y ofrece cifrado basado en software y hardware para mejorar la confidencialidad y la seguridad.  ONTAP cumple con FIPS 140-2 para todas las conexiones SSL.




== Público objetivo

Este documento está dirigido a tomadores de decisiones de IA, ingenieros de datos, líderes empresariales y ejecutivos departamentales que desean aprovechar una infraestructura diseñada para brindar soluciones empresariales RAG y GenAI.  El conocimiento previo de inferencia de IA, LLM, Kubernetes, redes y sus componentes ayudará durante la fase de implementación.



== Requisitos tecnológicos



=== Hardware



==== Tecnologías de inteligencia artificial de Intel

Con Xeon 6 como CPU host, los sistemas acelerados se benefician de un alto rendimiento de un solo subproceso, mayor ancho de banda de memoria, confiabilidad, disponibilidad y capacidad de servicio (RAS) mejoradas y más líneas de E/S.  Intel AMX acelera la inferencia para INT8 y BF16 y ofrece soporte para modelos entrenados con FP16, con hasta 2048 operaciones de punto flotante por ciclo por núcleo para INT8 y 1024 operaciones de punto flotante por ciclo por núcleo para BF16/FP16.  Para implementar una solución RAG utilizando procesadores Xeon 6, generalmente se recomienda una RAM mínima de 250 GB y 500 GB de espacio en disco.  Sin embargo, esto depende en gran medida del tamaño del modelo LLM.  Para obtener más información, consulte Intel https://www.intel.com/content/dam/www/central-libraries/us/en/documents/2024-05/intel-xeon-6-product-brief.pdf["Procesador Xeon 6"^] Descripción del producto.

Figura 1 - Servidor de cómputo con procesadores Intel Xeon 6image:aipod-mini-002.png["300.300"]



==== Almacenamiento AFF de NetApp

Los sistemas NetApp AFF Serie A de nivel básico y medio ofrecen mayor rendimiento, densidad y eficiencia.  Los sistemas NetApp AFF A20, AFF A30 y AFF A50 proporcionan un verdadero almacenamiento unificado que admite bloques, archivos y objetos, basado en un único sistema operativo que puede administrar, proteger y movilizar datos sin problemas para aplicaciones RAG al menor costo en la nube híbrida.

Figura 2 - Sistema NetApp AFF Serie A.image:aipod-mini-003.png["300.300"]

|===
| *Hardware* | *Cantidad* | *Comentario* 


| Servidor basado en Intel Xeon 6 | 2 | Nodos de inferencia RAG: con procesadores Intel Xeon serie 6900 o Intel Xeon serie 6700 de doble zócalo y de 250 GB a 3 TB de RAM con DDR5 (6400 MHz) o MRDIMM (8800 MHz).  Servidor 2U. 


| Servidor de plano de control con procesador Intel | 1 | Plano de control de Kubernetes/servidor 1U. 


| Elección de conmutador Ethernet de 100 Gb | 1 | Conmutador de centro de datos. 


| NetApp AFF A20 (o AFF A30; AFF A50) | 1 | Capacidad máxima de almacenamiento: 9.3PB.  Nota: Redes: puertos 10/25/100 GbE. 
|===
Para la validación de este diseño de referencia se utilizaron servidores con procesadores Intel Xeon 6 de Supermicro (222HA-TN-OTO-37) y un switch 100GbE de Arista (7280R3A).



=== Software



==== Plataforma abierta para IA empresarial

La Plataforma Abierta para IA Empresarial (OPEA) es una iniciativa de código abierto liderada por Intel en colaboración con socios del ecosistema.  Proporciona una plataforma modular de bloques de construcción componibles diseñados para acelerar el desarrollo de sistemas de IA generativa de vanguardia, con un fuerte enfoque en RAG.  OPEA incluye un marco integral que incluye LLM, almacenes de datos, motores de solicitud, planos arquitectónicos RAG y un método de evaluación de cuatro pasos que evalúa los sistemas de IA generativa en función del rendimiento, las características, la confiabilidad y la preparación empresarial.

En esencia, la OPEA consta de dos componentes clave:

* GenAIComps: un conjunto de herramientas basado en servicios compuesto por componentes de microservicios
* Ejemplos de GenAI: soluciones listas para implementar como ChatQnA que demuestran casos de uso prácticos


Para más detalles, consulte la https://opea-project.github.io/latest/index.html["Documentación del proyecto OPEA"^]



==== Inferencia de Intel AI para empresas impulsada por OPEA

OPEA para Intel AI for Enterprise RAG simplifica la transformación de los datos de su empresa en información útil.  Equipado con procesadores Intel Xeon, integra componentes de socios de la industria para ofrecer un enfoque optimizado para la implementación de soluciones empresariales.  Se escala sin problemas con marcos de orquestación probados, brindando la flexibilidad y la elección que su empresa necesita.

Basándose en la base de OPEA, Intel AI for Enterprise RAG amplía esta base con características clave que mejoran la escalabilidad, la seguridad y la experiencia del usuario.  Estas características incluyen capacidades de malla de servicio para una integración perfecta con arquitecturas modernas basadas en servicios, validación lista para producción para la confiabilidad de la canalización y una interfaz de usuario rica en funciones para RAG como servicio, lo que permite una fácil administración y monitoreo de los flujos de trabajo.  Además, el soporte de Intel y sus socios brinda acceso a un amplio ecosistema de soluciones, combinado con gestión de identidad y acceso (IAM) integrada con interfaz de usuario y aplicaciones para operaciones seguras y compatibles.  Las barandillas programables brindan un control detallado sobre el comportamiento de las tuberías, lo que permite configuraciones personalizadas de seguridad y cumplimiento.



==== ONTAP de NetApp

NetApp ONTAP es la tecnología fundamental que sustenta las soluciones de almacenamiento de datos críticos de NetApp.  ONTAP incluye varias funciones de gestión y protección de datos, como protección automática contra ransomware contra ciberataques, funciones de transporte de datos integradas y capacidades de eficiencia de almacenamiento.  Estos beneficios se aplican a una variedad de arquitecturas, desde locales hasta multicloud híbrido en NAS, SAN, objetos y almacenamiento definido por software para implementaciones LLM.  Puede utilizar un servidor de almacenamiento de objetos ONTAP S3 en un clúster ONTAP para implementar aplicaciones RAG, aprovechando la eficiencia de almacenamiento y la seguridad de ONTAP, proporcionada a través de usuarios autorizados y aplicaciones cliente.  Para obtener más información, consulte https://docs.netapp.com/us-en/ontap/s3-config/index.html["Obtenga más información sobre la configuración de ONTAP S3"^]



==== Trident de NetApp

El software NetApp Trident es un orquestador de almacenamiento de código abierto y totalmente compatible con contenedores y distribuciones de Kubernetes, incluido Red Hat OpenShift.  Trident funciona con todo el portafolio de almacenamiento de NetApp , incluido NetApp ONTAP y también admite conexiones NFS e iSCSI.  Para obtener más información, consulte https://github.com/NetApp/trident["NetApp Trident en Git"^]

|===
| *Software* | *Versión* | *Comentario* 


| OPEA para Intel AI para Enterprise RAG | 1.1.2 | Plataforma RAG empresarial basada en microservicios OPEA 


| Interfaz de almacenamiento de contenedores (controlador CSI) | NetApp Trident 25.02 | Permite el aprovisionamiento dinámico, copias Snapshot de NetApp y volúmenes. 


| Ubuntu | 22.04.5 | Sistema operativo en un clúster de dos nodos 


| Orquestación de contenedores | Kubernetes 1.31.4 | Entorno para ejecutar el marco RAG 


| ONTAP | ONTAP 9.16.1P4 | Sistema operativo de almacenamiento en AFF A20.  Cuenta con Vscan y ARP. 
|===


== Implementación de la solución



=== Pila de software

La solución se implementa en un clúster de Kubernetes que consta de nodos de aplicaciones basados en Intel Xeon.  Se requieren al menos tres nodos para implementar alta disponibilidad básica para el plano de control de Kubernetes.  Validamos la solución utilizando el siguiente diseño de clúster.

Tabla 3: Disposición del clúster de Kubernetes

|===
| Node | Role | Cantidad 


| Servidores con procesadores Intel Xeon 6 y 1 TB de RAM | Nodo de aplicación, nodo de plano de control | 2 


| Servidor genérico | Nodo del plano de control | 1 
|===
La siguiente figura muestra una "vista de la pila de software" de la solución.image:aipod-mini-004.png["600.600"]



=== Pasos de implementación



==== Implementar el dispositivo de almacenamiento ONTAP

Implemente y aprovisione su dispositivo de almacenamiento NetApp ONTAP .  Consulte la https://docs.netapp.com/us-en/ontap-systems-family/["Documentación de los sistemas de hardware de ONTAP"^] Para más detalles.



==== Configurar una SVM de ONTAP para acceso NFS y S3

Configure una máquina virtual de almacenamiento ONTAP (SVM) para acceso NFS y S3 en una red a la que puedan acceder sus nodos de Kubernetes.

Para crear una SVM usando ONTAP System Manager, navegue a Almacenamiento > Máquinas virtuales de almacenamiento y haga clic en el botón + Agregar.  Al habilitar el acceso S3 para su SVM, elija la opción para utilizar un certificado firmado por una CA externa (autoridad de certificación), no un certificado generado por el sistema.  Puede utilizar un certificado autofirmado o un certificado firmado por una CA de confianza pública.  Para obtener más detalles, consulte la https://docs.netapp.com/us-en/ontap/index.html["Documentación de ONTAP ."^]

La siguiente captura de pantalla muestra la creación de una SVM utilizando ONTAP System Manager.  Modifique los detalles según sea necesario en función de su entorno.

Figura 4 – Creación de SVM utilizando ONTAP System Manager.image:aipod-mini-005.png["600.600"] image:aipod-mini-006.png["600.600"]



==== Configurar permisos de S3

Configure los ajustes de usuario/grupo S3 para la SVM que creó en el paso anterior.  Asegúrese de tener un usuario con acceso completo a todas las operaciones de API S3 para esa SVM.  Consulte la documentación de ONTAP S3 para obtener más detalles.

Nota: Este usuario será necesario para el servicio de ingesta de datos de la aplicación Intel AI for Enterprise RAG.  Si creó su SVM usando ONTAP System Manager, System Manager habrá creado automáticamente un usuario llamado `sm_s3_user` y una política denominada `FullAccess` cuando creó su SVM, pero no se le habrán asignado permisos `sm_s3_user` .

Para editar los permisos de este usuario, vaya a Almacenamiento > Máquinas virtuales de almacenamiento, haga clic en el nombre de la SVM que creó en el paso anterior, haga clic en Configuración y, luego, haga clic en el ícono de lápiz junto a "S3".  Dar `sm_s3_user` acceso completo a todas las operaciones de la API de S3, crear un nuevo grupo que asocie `sm_s3_user` con el `FullAccess` política como se muestra en la siguiente captura de pantalla.

Figura 5 – Permisos de S3.

image:aipod-mini-007.png["600.600"]



==== Crear un bucket S3

Crea un bucket S3 dentro del SVM que creaste anteriormente.  Para crear una SVM usando ONTAP System Manager, navegue a Almacenamiento > Cubos y haga clic en el botón + Agregar.  Para obtener detalles adicionales, consulte la documentación de ONTAP S3.

La siguiente captura de pantalla muestra la creación de un depósito S3 mediante ONTAP System Manager.

Figura 6 – Crear un bucket S3.image:aipod-mini-008.png["600.600"]



==== Configurar los permisos del bucket S3

Configure los permisos para el depósito S3 que creó en el paso anterior.  Asegúrese de que el usuario que configuró en un paso anterior tenga los siguientes permisos: `GetObject, PutObject, DeleteObject, ListBucket, GetBucketAcl, GetObjectAcl, ListBucketMultipartUploads, ListMultipartUploadParts, GetObjectTagging, PutObjectTagging, DeleteObjectTagging, GetBucketLocation, GetBucketVersioning, PutBucketVersioning, ListBucketVersions, GetBucketPolicy, PutBucketPolicy, DeleteBucketPolicy, PutLifecycleConfiguration, GetLifecycleConfiguration, GetBucketCORS, PutBucketCORS.`

Para editar los permisos de un depósito S3 mediante ONTAP System Manager, navegue a Almacenamiento > Depósitos, haga clic en el nombre de su depósito, haga clic en Permisos y, luego, haga clic en Editar.  Consulte la https://docs.netapp.com/us-en/ontap/object-storage-management/index.html["Documentación de ONTAP S3"^] Para más detalles.

La siguiente captura de pantalla muestra los permisos de depósito necesarios en ONTAP System Manager.

Figura 7 – Permisos del bucket S3.image:aipod-mini-009.png["600.600"]



==== Crear una regla de uso compartido de recursos de origen cruzado de bucket

Con la CLI de ONTAP , cree una regla de uso compartido de recursos de origen cruzado (CORS) para el depósito que creó en un paso anterior:

[source, cli]
----
ontap::> bucket cors-rule create -vserver erag -bucket erag-data -allowed-origins *erag.com -allowed-methods GET,HEAD,PUT,DELETE,POST -allowed-headers *
----
Esta regla permite que la aplicación web OPEA para Intel AI for Enterprise RAG interactúe con el depósito desde un navegador web.



==== Implementar servidores

Implemente sus servidores e instale Ubuntu 22.04 LTS en cada servidor.  Después de instalar Ubuntu, instale las utilidades NFS en cada servidor.  Para instalar las utilidades NFS, ejecute el siguiente comando:

[source, cli]
----
 apt-get update && apt-get install nfs-common
----


==== Instalar Kubernetes

Instale Kubernetes en sus servidores usando Kubespray.  Consulte la https://kubespray.io/["Documentación de Kubespray"^] Para más detalles.



==== Instalar el controlador Trident CSI

Instale el controlador CSI Trident de NetApp en su clúster de Kubernetes.  Consulte la https://docs.netapp.com/us-en/trident/trident-get-started/kubernetes-deploy.html["Documentación de instalación de Trident"^] Para más detalles.



==== Crear un back end de Trident

Cree un back end Trident para el SVM que creó anteriormente.  Al crear su back end, utilice el `ontap-nas` conductor.  Consulte la https://docs.netapp.com/us-en/trident/trident-use/ontap-nas.html["Documentación del backend de Trident"^] Para más detalles.



==== Crear una clase de almacenamiento

Cree una clase de almacenamiento de Kubernetes correspondiente al back end de Trident que creó en el paso anterior.  Consulte la documentación de la clase de almacenamiento Trident para obtener más detalles.



==== OPEA para Intel AI para Enterprise RAG

Instale OPEA para Intel AI for Enterprise RAG en su clúster de Kubernetes.  Consulte la https://github.com/opea-project/Enterprise-RAG/blob/release-1.2.0/deployment/README.md["Implementación de Intel AI for Enterprise RAG"^] documentación para más detalles.  Asegúrese de tomar nota de las modificaciones necesarias del archivo de configuración que se describen más adelante en este documento.  Debe realizar estas modificaciones antes de ejecutar el manual de instalación para que la aplicación Intel AI for Enterprise RAG funcione correctamente con su sistema de almacenamiento ONTAP .



=== Habilitar el uso de ONTAP S3

Al instalar OPEA para Intel AI for Enterprise RAG, edite su archivo de configuración principal para habilitar el uso de ONTAP S3 como su repositorio de datos de origen.

Para habilitar el uso de ONTAP S3, configure los siguientes valores dentro del `edp` sección.

Nota: De forma predeterminada, la aplicación Intel AI for Enterprise RAG ingiere datos de todos los depósitos existentes en su SVM.  Si tiene varios depósitos en su SVM, puede modificarlos `bucketNameRegexFilter` campo para que los datos se ingieran solo desde ciertos grupos.

[source, cli]
----
edp:
  enabled: true
  namespace: edp
  dpGuard:
    enabled: false
  storageType: s3compatible
  s3compatible:
    region: "us-east-1"
    accessKeyId: "<your_access_key>"
    secretAccessKey: "<your_secret_key>"
    internalUrl: "https://<your_ONTAP_S3_interface>"
    externalUrl: "https://<your_ONTAP_S3_interface>"
    bucketNameRegexFilter: ".*"
----


=== Configurar los ajustes de sincronización programada

Al instalar la aplicación OPEA para Intel AI for Enterprise RAG, habilite `scheduledSync` para que la aplicación ingiera automáticamente archivos nuevos o actualizados desde sus depósitos S3.

Cuando `scheduledSync` está habilitado, la aplicación verifica automáticamente sus buckets S3 de origen en busca de archivos nuevos o actualizados.  Cualquier archivo nuevo o actualizado que se encuentre como parte de este proceso de sincronización se incorpora automáticamente y se agrega a la base de conocimiento de RAG.  La aplicación verifica sus depósitos de origen según un intervalo de tiempo preestablecido.  El intervalo de tiempo predeterminado es de 60 segundos, lo que significa que la aplicación comprueba si hay cambios cada 60 segundos.  Es posible que desee cambiar este intervalo para adaptarlo a sus necesidades específicas.

Para habilitar `scheduledSync` y establezca el intervalo de sincronización, configure los siguientes valores en `deployment/components/edp/values.yaml:`

[source, cli]
----
celery:
  config:
    scheduledSync:
      enabled: true
      syncPeriodSeconds: "60"
----


=== Cambiar los modos de acceso al volumen

En `deployment/components/gmc/microservices-connector/helm/values.yaml` , para cada volumen en el `pvc` lista, cambiar el `accessMode` a `ReadWriteMany` .

[source, cli]
----
pvc:
  modelLlm:
    name: model-volume-llm
    accessMode: ReadWriteMany
    storage: 100Gi
  modelEmbedding:
    name: model-volume-embedding
    accessMode: ReadWriteMany
    storage: 20Gi
  modelReranker:
    name: model-volume-reranker
    accessMode: ReadWriteMany
    storage: 10Gi
  vectorStore:
    name: vector-store-data
    accessMode: ReadWriteMany
    storage: 20Gi
----


=== (Opcional) Deshabilitar la verificación del certificado SSL

Si utilizó un certificado autofirmado al habilitar el acceso S3 para su SVM, debe deshabilitar la verificación del certificado SSL.  Si utilizó un certificado firmado por una CA de confianza pública, puede omitir este paso.

Para deshabilitar la verificación del certificado SSL, configure los siguientes valores en `deployment/components/edp/values.yaml:`

[source, cli]
----
edpExternalUrl: "https://s3.erag.com"
edpExternalSecure: "true"
edpExternalCertVerify: "false"
edpInternalUrl: "edp-minio:9000"
edpInternalSecure: "true"
edpInternalCertVerify: "false"
----


==== Acceda a OPEA para Intel AI para la interfaz de usuario RAG empresarial

Acceda a la interfaz de usuario RAG de OPEA para Intel AI for Enterprise.  Consulte la https://github.com/opea-project/Enterprise-RAG/blob/release-1.1.2/deployment/README.md#interact-with-chatqna["Documentación de implementación de Intel AI for Enterprise RAG"^] Para más detalles.

Figura 8: UI de OPEA para Intel AI para Enterprise RAG.image:aipod-mini-010.png["600.600"]



==== Ingerir datos para RAG

Ahora puede ingerir archivos para incluirlos en la ampliación de consultas basada en RAG.  Existen múltiples opciones para ingerir archivos.  Elija la opción adecuada a sus necesidades.

Nota: después de ingerir un archivo, la aplicación OPEA para Intel AI for Enterprise RAG busca automáticamente actualizaciones del archivo y las ingiere según corresponda.

*Opción 1: Cargar directamente a su bucket S3 Para ingerir muchos archivos a la vez, le recomendamos cargar los archivos a su bucket S3 (el bucket que creó anteriormente) utilizando el cliente S3 de su elección.  Los clientes S3 populares incluyen AWS CLI, Amazon SDK para Python (Boto3), s3cmd, S3 Browser, Cyberduck y Commander One.  Si los archivos son de un tipo compatible, cualquier archivo que cargue en su bucket S3 será ingerido automáticamente por la aplicación OPEA para Intel AI for Enterprise RAG.

Nota: al momento de escribir este artículo, se admiten los siguientes tipos de archivos: PDF, HTML, TXT, DOC, DOCX, PPT, PPTX, MD, XML, JSON, JSONL, YAML, XLS, XLSX, CSV, TIFF, JPG, JPEG, PNG y SVG.

Puede utilizar la interfaz de usuario RAG de OPEA para Intel AI for Enterprise para confirmar que sus archivos se ingirieron correctamente.  Consulte la documentación de la interfaz de usuario de Intel AI for Enterprise RAG para obtener más detalles.  Tenga en cuenta que la aplicación puede tardar un tiempo en procesar una gran cantidad de archivos.

*Opción 2: Cargar usando la interfaz de usuario Si necesita ingerir solo una pequeña cantidad de archivos, puede ingerirlos usando la interfaz de usuario RAG de OPEA para Intel AI for Enterprise.  Consulte la documentación de la interfaz de usuario de Intel AI for Enterprise RAG para obtener más detalles.

Figura 9 – Interfaz de usuario de ingesta de datos.image:aipod-mini-011.png["600.600"]



==== Ejecutar consultas de chat

Ahora puedes "chatear" con la aplicación OPEA para Intel AI for Enterprise RAG mediante la interfaz de usuario de chat incluida.  Al responder a sus consultas, la aplicación realiza RAG utilizando sus archivos ingeridos.  Esto significa que la aplicación busca automáticamente información relevante dentro de los archivos ingeridos e incorpora esta información al responder a sus consultas.



== Guía de tallas

Como parte de nuestro esfuerzo de validación, realizamos pruebas de rendimiento en coordinación con Intel.  Esta prueba dio como resultado la guía de tamaño que se describe en la siguiente tabla.

|===
| Caracterizaciones | Valor | Comentario 


| Tamaño del modelo | 20 mil millones de parámetros | Llama-8B, Llama-13B, Mistral 7B, Qwen 14B, DeepSeek Distill 8B 


| Tamaño de entrada | ~2k tokens | ~4 páginas 


| Tamaño de salida | ~2k tokens | ~4 páginas 


| Usuarios concurrentes | 32 | "Usuarios concurrentes" se refiere a solicitudes que envían consultas al mismo tiempo. 
|===
_Nota: La guía de tamaño presentada anteriormente se basa en la validación del rendimiento y en los resultados de pruebas recopilados utilizando procesadores Intel Xeon 6 con 96 núcleos.  Para clientes con tokens de E/S y requisitos de tamaño de modelo similares, recomendamos utilizar servidores con procesadores Xeon 6 con 96 o 128 núcleos.



== Conclusión

Los sistemas RAG empresariales y LLM son tecnologías que trabajan juntas para ayudar a las organizaciones a brindar respuestas precisas y adaptadas al contexto.  Estas respuestas implican la recuperación de información basada en una vasta colección de datos empresariales privados e internos.  Al utilizar RAG, API, incrustaciones vectoriales y sistemas de almacenamiento de alto rendimiento para consultar repositorios de documentos que contienen datos de la empresa, los datos se procesan de forma más rápida y segura.  NetApp AIPod Mini combina la infraestructura de datos inteligente de NetApp con las capacidades de gestión de datos de ONTAP y los procesadores Intel Xeon 6, Intel AI for Enterprise RAG y la pila de software OPEA para ayudar a implementar aplicaciones RAG de alto rendimiento y poner a las organizaciones en el camino hacia el liderazgo en IA.



== Reconocimiento

Este documento es trabajo de Sathish Thyagarajan y Michael Ogelsby, miembros del equipo de ingeniería de soluciones de NetApp .  Los autores también desean agradecer al equipo de productos de inteligencia artificial empresarial de Intel (Ajay Mungara, Mikolaj Zyczynski, Igor Konopko, Ramakrishna Karamsetty, Michal Prostko, Shreejan Mistry y Ned Fiori) y a otros miembros del equipo de NetApp(Lawrence Bunka, Bobby Oommen y Jeff Liborio) por su continuo apoyo y ayuda durante la validación de esta solución.



== Lista de materiales

La siguiente fue la lista de materiales utilizada para la validación funcional de esta solución y puede usarse como referencia.  Se podría utilizar cualquier servidor o componente de red (o incluso una red existente con un ancho de banda preferiblemente de 100 GbE) que se ajuste a la siguiente configuración.

Para el servidor de aplicaciones:

|===
| *Número de pieza* | *Descripción del Producto* | *Cantidad* 


| 222HA-TN-OTO-37 | Hyper SuperServer SYS-222HA-TN /2U | 2 


| P4X-GNR6972P-SRPL2-UCC | Intel Xeon 6972P 2P 128C 2G 504M 500W SGX512 | 2 


| RAM | MEM-DR564MC-ER64(x16) 64 GB DDR5-6400 2RX4 (16 GB) ECC RDIMM | 32 


|  | HDS-M2N4-960G0-E1-TXD-NON-080(x2) SSD M.2 NVMe PCIe4 960GB 1DWPD TLC D, 80 mm | 2 


|  | Fuente de alimentación de salida única redundante WS-1K63A-1R(x2)1U 692W/1600W.  Disipación de calor de 2361 BTU/hora con temperatura máxima de 59 C (aprox.) | 4 
|===
Para el servidor de control:

|===


| *Número de pieza* | *Descripción del Producto* | *Cantidad* 


| 511R-M-OTO-17 | OPTIMIZADO HASTA 1U X13SCH-SYS, CSE-813MF2TS-R0RCNBP, PWS-602A-1R | 1 


| P4X-GNR6972P-SRPL2-UCC | P4D-G7400-SRL66(x1) ADL Pentium G7400 | 1 


| RAM | MEM-DR516MB-EU48(x2)16GB DDR5-4800 1Rx8 (16Gb) ECC UDIMM | 1 


|  | HDS-M2N4-960G0-E1-TXD-NON-080(x2) SSD M.2 NVMe PCIe4 960GB 1DWPD TLC D, 80 mm | 2 
|===
Para el conmutador de red:

|===


| *Número de pieza* | *Descripción del Producto* | *Cantidad* 


| DCS-7280CR3A | Arista 7280R3A 28x100 GbE | 1 
|===
Almacenamiento AFF de NetApp :

|===


| *Número de pieza* | *Descripción del Producto* | *Cantidad* 


| AFF-A20A-100-C | Sistema AFF A20 HA, -C | 1 


| X800-42U-R6-C | Cable de puente, en cabina, C13-C14, -C | 2 


| X97602A-C | Fuente de alimentación, 1600 W, titanio, -C | 2 


| X66211B-2-N-C | Cable, 100 GbE, QSFP28-QSFP28, Cu, 2 m, -C | 4 


| X66240A-05-N-C | Cable, 25 GbE, SFP28-SFP28, Cu, 0,5 m, -C | 2 


| X5532A-N-C | Riel, 4 postes, delgado, agujero redondo/cuadrado, pequeño, ajustable, 24-32, -C | 1 


| X4024A-2-A-C | Paquete de unidades 2X1,92 TB, NVMe4, SED, -C | 6 


| X60130A-C | Módulo de E/S, 2PT, 100 GbE, -C | 2 


| X60132A-C | Módulo de E/S, 4 PT, 10/25 GbE, -C | 2 


| SW-ONTAPB-FLASH-A20-C | SW, paquete básico de ONTAP , por TB, Flash, A20, -C | 23 
|===


== Dónde encontrar información adicional

Para obtener más información sobre la información que se describe en este documento, revise los siguientes documentos y/o sitios web:

https://www.netapp.com/support-and-training/documentation/ONTAP%20S3%20configuration%20workflow/["Documentación de productos de NetApp"^]

link:https://github.com/opea-project/Enterprise-RAG/tree/main["Proyecto OPEA"]

https://github.com/opea-project/Enterprise-RAG/tree/main/deployment/playbooks["Manual de implementación de OPEA Enterprise RAG"^]
